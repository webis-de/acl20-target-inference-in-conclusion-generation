{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "\n",
    "from importance_estimation import ImportanceEstimationModel\n",
    "from   matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train arguments: 300\n",
      "Number of test arguments: 52\n",
      "Number of dev arguments: 49\n"
     ]
    }
   ],
   "source": [
    "model = ImportanceEstimationModel()\n",
    "train_data, dev_data, test_data = model.load_data('../data/student_essays/main/train_tagged.json', \n",
    "                                                  '../data/student_essays/main/valid_tagged.json', \n",
    "                                                  '../data/student_essays/main/test_tagged.json')\n",
    "\n",
    "print('Number of train arguments:', len(train_data))\n",
    "print('Number of test arguments:', len(test_data))\n",
    "print('Number of dev arguments:', len(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(filter(lambda x: len(x['claims']) > 0, train_data))\n",
    "test_data = list(filter(lambda x: len(x['claims']) > 0, test_data))\n",
    "dev_dta = list(filter(lambda x: len(x['claims']) > 0, dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miladalshomary/Development/thirdparty/anaconda/lib/python3.7/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y = model.feature_representation(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape : (942, 8)\n",
      "train_Y shape : (942,)\n"
     ]
    }
   ],
   "source": [
    "print('train_X shape :', train_X.shape)\n",
    "print('train_Y shape :', train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only train on small part of train_X\n",
    "# idx = np.random.choice(np.arange(len(train_X)), 2000, replace=False)\n",
    "# train_sample_X = train_X[idx]\n",
    "# train_sample_Y = train_Y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_absolute_error = model.train_svr(train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: -0.10899280381915595\n"
     ]
    }
   ],
   "source": [
    "print('Mean absolute error:', mean_absolute_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluation we compute the __mean reciprocal rank (MRR)__. A document is relievant if it overlaps with the conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9775641025641024"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.mrr_evaluation(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(filter(lambda x: len(x['claims']) > 0, train_data))\n",
    "test_data = list(filter(lambda x: len(x['claims']) > 0, test_data))\n",
    "dev_data = list(filter(lambda x: len(x['claims']) > 0, dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_train_data = model.score_data(train_data)\n",
    "scored_test_data = model.score_data(test_data)\n",
    "scored_dev_data = model.score_data(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json.dump(scored_train_data, open('../data/student_essays/main/train_tagged.json', 'w'))\n",
    "json.dump(scored_test_data, open('../data/student_essays/main/test_tagged.json', 'w'))\n",
    "json.dump(scored_dev_data, open('../data/student_essays/main/valid_tagged.json', 'w'))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
